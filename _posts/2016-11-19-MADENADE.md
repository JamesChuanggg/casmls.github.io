---
layout: post
title: "Autoregressive Distribution Estimation"
categories: general
author: Gamaleldin Elsayed
excerpt_separator: <!--more-->
comments: true
---
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

This week we discussed (Germain et al., MADE[1]) and (Uria et al., NADE[2]) papers on autoregressive distribution estimation. The main approach to estimate joint distributions of data is similar in the two papers. That is to modify the structure of the autoencoder neural networks to be consistent with autoregressive models. MADE focused on binary data cases whereas NADE extended this idea to real valued data, explored more interesting network architectures, and peformed more extensive experiments.

<!--more-->

# Distribution Estimation
## Set up

Given a set of examples $$\{x^{(t)}\}_{t=1}^T$$, where $$x \in R^D$$, the goal is to estimate a joint distribution $$p(x)$$. This distribution quantifies the statistical properties of data and can be used as a generative model to sample new data. This generative model is useful in many applications such as classifications, data denoising or missing data completion. This problem is relatively easy if the dimensionality of the data is low (_e.g.,_ estimate distribution from many examples of real valued numbers). However, in cases when data is high dimensional (_e.g.,_ space of pixels of an image), estimation of the data distribution becomes difficult. The main problem is that as dimensionality increases the volume of the space the distribution $$p(x)$$ needs to cover increases exponentially, which make it harder for finite datasets to give a clear picture of the statistical properties of that space. 

## Autoencoder neural networks:
One powerful idea to estimate the distribution of data is to utilize the power of neural networks as function approximators. In this setting, a neural network learns a feedforward representation of the input data examples in its hidden layers with the goal of regenerating the input data as accurate as possible. These hidden representations can thus reveal the statistical structures of the data generative distribution. For example, to learn the representation of binary data using a one-layer network, we can frame the problem as follows:

$$
h(x) = g(b + Wx)\\
\hat x = sigmoid(c + V h(x))
$$

where $$g$$ is the hidden layer nonlinear activation function, W and V are network input-to-hidden and hidden-to-output weights, respectively, and b and c are the bias terms. The main advantage of this framework is that it is very flexible and easy to train to find the best parameters $$\theta = \{W, V, b, c\}$$ with stochastic gradient descent. The typical loss function used if data is binary is the cross-entropy:

$$
l(x) = \sum_{d=1}^D -x_d \log(\hat {x}_d) - (1-x_d) \log(1 - \hat {x}_d)
$$

This loss is thought to represent the log likelihood function. However, in the general case of fully connected network, it is in fact not a proper loglikelihood. The main confound is that in a fully connected network the generative process of data at dimension $$d$$ depends on the input data at dimensions $$d$$. This makes it possible for the network to learn trivial generative processes such as simply copying the input data to the output (i.e., there is a trivial set of weight that will simply assign $$x_d$$ to $$\hat {x}_d$$). The essence of this problem is that the distribution identified is not truly the joint distributions (not properly normalized). One can see that in this trivial case of copying the input where the data is produced perfectly with probability one for each sample data, which makes the probabilities over all possible patterns does not sum to 1 (equal to number of training data patterns in this example).


## Distribution estimation with autoregression:
The decomposition of joint distributions to product of conditions gives a solution to the above problem. In general, the joint distribution over data can be written in the form of conditional product as follows:

$$
p(x_1, ..., x_D) = p(x_D \, | \, x_1, ..., x_{D-1}) p(x_{D-1} \, | \, x_1,..., x_{D-2}) ... p(x_1) = \prod_{d=1}^{D} p(x_d \, | \, x_{<d})
$$

Remember the main problem of the autoencoders is that $$\hat x_d$$ depends on  all $$xs$$ due to the full connections in the neural network. However, if the connections are modified to satisfy this autoregressive property, this will eleminate the possibilities of trivial representations and will allow the network to learn a proper joint distribution. The loss function becomes then a valid negative log-likelihood:

$$
l(x) = - \log p(x_1, ..., x_D) = \sum_{d=1}^D - \log p(x_d \, | \, x_{<d}) = \sum_{d=1}^D -x_d \log p(x_d = 1 \, | \, x_{<d})  - (1-x_d) \log p(x_d = 0 \, | \, x_{<d}) 
$$


### Masked Autoencoder for Distribution Estimation (MADE)

The main idea of MADE is to modify the connections in the autoencoder to satisfy the autoregressive property using masked weights. To enforce that there are no dependency between $$\hat {x}_d$$ and $$x_{>=d}$$, MADE ensures there is no computational paths between $$\hat {x}_d $$ and $$x_{>=d}$$ by multiply the network weight by masks as follows:

$$
h(x) = g(b + (M^W \odot W) x)\\
\hat x = sigmoid(c + (M^V \odot V) h(x))
$$

where $$M^W$$ and $$M^V$$ are mask matrices. The marix product of $$M^W$$ and $$M^V$$ represents the number of computational paths from the input to the output in this one-layer network. Thus, to satisfy the autoregressive property, we need to choose $$M^W$$ and $$M^V$$ such that the matrix $$M^{WV} = M^W M^V$$ is lower triangle. That is there is no computational paths between $$\hat {x}_d$$ and $$x_{>=d}$$. The same framework generalizes to deep networks with more than 1 hidden layers by ensuring the product of the masks have a lower triagnular structure (Figure 1). The procedure is detailed in algorithm 1. MADE focused entirely on estimation the distribution of only binary data.

<!--
![ABC]({{site.base_url}}/img/MADE/Fig1.png)
![ABC]({{site.base_url}}/img/MADE/Alg1.png)
-->
![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/MADE/Fig1.png)
![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/MADE/Alg1.png)


### Neural Auroregressive Distribution Estimation (NADE)
The same approach of utilizing the autoregressive property by modifying the autoencoder weights is also adopted in NADE except that NADE uses fixed set of masks (NADE alogorithm 1 and Figure 1) whereas in MADE masks are allowed to change. 
<!--
![ABC]({{site.base_url}}/img/NADE/Fig1.png)
![ABC]({{site.base_url}}/img/NADE/Alg1.png)
-->

![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/NADE/Fig1.png)
![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/NADE/Alg1.png)

One major extension of NADE over MADE is its ability to handle real valued data by modifying the model to the Gaussian-RBM (Welling et al., [3]). That is each of the conditionals is modeled by a mixture of Gaussians as follows:


$$
p(x_{o_d} \, | \, x_{o_{<d}}) = \sum_{c=1}^C \pi_{o_d,c} \mathcal{N} (x_{o_d}; \mu_{o_d,c}, \sigma_{o_d,c}^2)\\
\pi_{o_d,c} = \frac{\exp\{z_{o_d,c}^{(\pi)}\}}{\sum_{c=1}^C\exp\{z_{o_d,c}^{(\pi)}\}}\\
\mu_{o_d,c} = z_{o_d,c}^{(\pi)}\\
\sigma_{o_d,c} = \exp\{z_{o_d,c}^{(\sigma)}\}\\
z_{o_d,c}^{(\pi)} = b_{o_d,c}^{(\pi)} + \sum_{k=1}^H V_{o_d,k,c}^{(\pi)} h_{d,k}\\
z_{o_d,c}^{(\mu)} = b_{o_d,c}^{(\mu)} + \sum_{k=1}^H V_{o_d,k,c}^{(\mu)} h_{d,k}\\
z_{o_d,c}^{(\sigma)} = b_{o_d,c}^{(\sigma)} + \sum_{k=1}^H V_{o_d,k,c}^{(\sigma)} h_{d,k}
$$


In deep architechtures, NADE used masks in a slightly different way than MADE. That is the input to network is the concatination of the masked data and the mask iteself (Figure 2). This allows the network to identify cases when input data is truly zero from cases when input data is zero because of the mask. NADE also explored other autoencoder architectures such as convolutional neural networks (Figure 3).
<!--
 ![ABC]({{site.base_url}}/img/NADE/Fig2.png)
 ![ABC]({{site.base_url}}/img/NADE/Fig3.png)
 -->

![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/NADE/Fig2.png)
![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/NADE/Fig3.png)

### Results:
**MADE** 

MADE was trained on UCI binary datasets using stochastic gradient descent with mini-batches of size 100 and a lookahead of 30 for early stopping. The results are quantified by the average negative-likelihood on the test set of each data.  

The results for the UCI data shows that MADE is the best performing model on almost half of the tested datasets (Table 4).
<!--
![ABC]({{site.base_url}}/img/NADE/Table4.png)
  -->
 ![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/MADE/Table4.png)

**NADE** 

NADE has more extensive experimental section. Here, I discuss the results on the UCI datasets claissfication experiments. Table 2 in NADE compares the log likelihood performance to other datasets and to also MADE. In these experiments, both NADE and MADE were better than other models in many cases. The performance of NADE and MADE were similar in almost all the datasets, but NADE was slightly better.

<!--
 ![ABC]({{site.base_url}}/img/NADE/Table2.png)
 -->
![ABC](/Users/gamalamin/git_local_repository/casmls.github.io/img/NADE/Table2.png)


## Conclusions
Both NADE and MADE are methods motivated by the idea of modeling valid distributions using autoregressive property. The two methods modifies autoencoder networks to enforce the autoregressive property on the network weights. The two methods successfully identify valid joint distributions while avoiding trivial solutions in traditional autoencoder such as copying inputs to outputs. NADE takes the idea of autoregressive models one step further by additionally estimating the distributions of non-binary data and to other network architecture like convolutional networks.


### References
[1] Germain, Mathieu, et al. “MADE: masked autoencoder for distribution estimation.” International Conference on Machine Learning. 2015. [link](http://www.jmlr.org/proceedings/papers/v37/germain15.pdf)

[2] Uria, Benigno, et al. “Neural Autoregressive Distribution Estimation.” arXiv preprint arXiv:1605.02226 (2016). [link](https://arxiv.org/pdf/1605.02226.pdf)

[3] Max Welling, Michal Rosen-Zvi, and Geoffrey E. Hinton. Exponential family harmoniums
with an application to information retrieval. In Advances in Neural Information Processing Systems 17, pages 1481–1488. MIT Press, 2005.[link](https://www.ics.uci.edu/~welling/publications/papers/GenHarm3.pdf)


