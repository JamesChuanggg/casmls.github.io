latent states evolve according to

$$
z_{r1} \sim \mathcal{N}(\mu_1, Q_1),
$$

$$
z_{r(t+1)} \mid z_{rt} \sim \mathcal{N}(A z_{rt}, Q)
$$

where $$A \in \mathbb{R}^{m \times m}$$, and $$Q_1, Q$$ are covariance matrices. In fLDS, the observation model is given by

$$
x_{rti} | z_{rt} \sim \mathcal{P}_{\lambda}\left( \lambda_{rti} = \left[ f_{\psi}(z_{rt}) \right]_i \right),
$$

where

* $$\mathcal{P}_{\lambda}(\lambda)$$ is a noise model with parameter $$\lambda$$,
* $$f_{\psi} : \mathbb{R}^m \to \mathbb{R}^n$$ is an arbitrary continuous function.

Here, $$f_{\psi}(\cdot)$$ is represented through a feed-forward neural network model, so the parameters $$\psi$$ amount to weights/biases
across units and layers. The aim is to then learn all the generative model parameters, which are denoted by $$\theta$$.

## Fitting via Auto-encoding Variational Bayes (AEVB)

Due to the noise model and the non-linear observation model, the posterior $$p_{\theta}(z | x)$$ is intractable and so the authors 
employ a stochastic variational inference approach. The posterior is then approximated by a tractable distribution $$q_{\phi}(z | x)$$
depending on parameters $$\phi$$ and the corresponding evidence lower bound $$\mathcal{L}$$ (ELBO) is maximized; here, an auto-encoding variational Bayes
approach is used to estimate $$\nabla \mathcal{L}$$. Given these, the ELBO is then maximized via stochastic gradient ascent.

This approach ends up being similar to that of the first paper, when considering the choice of approximate posterior distributions used. Here,
a Gaussian approximation is used:

$$
q_{\phi}(z_r | x_r) = \mathcal{N}(z_r | \mu_{\phi}(x_r), \Sigma_{\phi}(x_r) ),
$$

where $$\mu_{\phi}(x_r)$$ and $$\Sigma_{\phi}(x_r)$$ are parameterized by the observations through a structured neural network. This
idea is similar to that in the SVAE paper, where we approximate the observation model by an exponential family conjugate to the model for
the local latent variables with a diverse class of potential functions; here, we approximate the posterior for the latent variables
by the conjugate family to that of the prior with a similarly diverse class of potential functions.

## Experiments

The above model is applied to various simulation and real data; we briefly describe one of the real data examples provided
in the paper. Here, they look at neural data obtained from an anesthetized macaque, which was taken while the monkey watched a movie of a
sinusoidal grating drifting in one of 72 orientations (starting from 0 degrees and incrementing in 5 degree steps). They then compare 
PfLDS (a fLDS with Poisson noise model) with a PLDS (Poisson LDS) using both a single orientation (0 degrees) and the full data, using 
a fraction of the data for training before assessing model fitting by one-step-ahead prediction on a held-out dataset. The figure below
illustrates the performance of the two models when looking at single-orientation data; we can see that PLDS requires 10 latent dimensions
to obtain the same predictive performance as a PfLDS with 3 latent dimensions - the PfLDS leads to more interpretable latent-variable
representations.

![fLDSRes1]({{site.base_url}}/img/flds1.png)

For the full orientation data, the below figure illustrates this principle again - that it provides better predictive performance with a
small number of latent dimensions.

![fLDSRes2]({{site.base_url}}/img/flds2.png)


### References

[1] Johnson, Matthew J., et al. “Composing graphical models with neural networks for structured representations and fast inference.” _Advances in Neural Information Processing Systems_ (2016). [link](http://arxiv.org/pdf/1603.06277)

[2] Gao, Yuanjun, et al. “Linear dynamical neural population models through nonlinear embeddings.” _Advances in Neural Information Processing Systems_ (2016). [link](http://arxiv.org/pdf/1603.06277)
