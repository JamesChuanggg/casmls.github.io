---
layout: post
title: "GAN: the new progresses"
categories: general
author: scott linderman
excerpt_separator: <!--more-->
comments: true
---

In this week's session, Yixin has led the discussion of two papers about GANs. The first paper, 
"Generalization and Equilibrium in Generative Adversarial Nets" by Arora et al. [1],  is a theoretical investigation 
of GANs, and the second paper, "Improved Training of Wasserstein GANs" by Gulrajani et al. [2], gives an new training 
method of Wasserstein GAN.  
<!--more-->

# GAN and Wasserstein Gan
GAN training is a two-player game in which the generator minimizes the divergence between its generative distribution 
and the data distribution while the discriminator tries to distinguish the samples from the generator's distribution and 
the real data samples. We say the generator "wins" when the discriminator performs no better than random guess. 

The optimization problem of the baic GAN is a min-max problem, 

$$
min_{G} max_{D} E_{x \sim p_{data}} [\log D(x)] + E_{ h \sim p_{Normal}} [\log (1 - D( G(h))]
$$

In another understanding, the best discriminator gives a divergence measure between the generator's distribution 
$$G(h), h \sim p_{Normal}$$, and the data distribution, $$p_{data}$$. If we have $$p_{data}(x)$$ and the discrinator is allowed to be any function, then the generator is 
optimized to minimize the Jesen-Shannon divergence between $$p_{data}$$ and $$G(h)$$. 

People then use Wasserstein distance to measure the divergence between two distributions (See Robin's [post]{https://casmls.github.io/general/2017/02/23/modified-gans.html} on 
Wasserstein GAN). The Wasserstein distance between the data distribution and the generative distribution is 
$$
sup_{f: 1 Lipschitz}  E_{x \sim p_{data}}[f(x)] - E_{x \sim p_{G(h)}}[f(x)] 
$$
Here the $f$ is the discriminator and takes the form of neural network. It is learned from GAN training. The objective is 
to minimize the Wasserstein's distance between these two distributions. 


The first paper works on the following problems:
1. The divergence measure is defined on distributions, but it is calculated by the discriminator on finite samples. 
   How large is the gap?
2. Can the training reach equilibrium?
3. What does it mean whether when the equilibrium is reached?

The second paper works on the problem of 
4. penalize the optimizer so that it searches the optimal discriminator approximately in the 1-Lipschitz space. 


# Generalization of the distance measure 

When sample size is not large enough, the distance of a distribution and the empirical distribution of its samples 
is non-zero. 

The two empirical distributions of two samples from the same distribution have a distance greater than zero. 

New distance measure neural network divergence 

d(NN)()

Then the paper shows the distance calculated on empirical distributions is similar to that calculated on real 
applications when the sample size is large. 


# equilibrium

Intuition: Powerful generator can is able to win, since it can arbitrarily minimize the ditance between the generative 
distribution and the target distribution. 

A less powerful generator can also win the game, but it need to be more powerful enough. 

The payoff of the game between the generator and the discriminator, 
F(u, v)

Mixed strategy always admits equilibrium. 

However, intinite mixture is not computable, so the paper proposed $\epsilon$-approximate equilibrium. 


The paper shows that, given enough mixtures of generator and discriminators, the generator can approximately win 
the game.

# MIX+GAN: Mixture of generators and discriminators

Minimize $T$ generators and their mixture weights, $T$ discrinimators and their mixture weights.  



# Wasserstein GAN training with gradient penalty (Paper 2)

The paper is based on the nice result that the optimal discriminator (called "critic" in the paper) has 
gradients with norm 1 almost everywhere. Here the gradient is  with respect to $x$, not the parameter of 
the discriminator. 

Gradient Clipping does not work very well for the following reasons. 

1. The optimizer with gradient clipping searches the discriminator in a space smaller than 1-Lipschitz, 
so it biases the discriminator toward simpler functions. 

2. Clipped gradients vanishe or explode as it backpropagate through network layers. 


The theoretical result of the gradient and the drawback of gradient clipping motivates the new method, gradient penalty, 
in the paper. The discriminator gets a penalty if the norm of its gradient is not 1. The objective is 

$$
L = \mathbb{E}_{\tilde{x} \sim p_G}[D(\tilde{x})] - E_{x \sim p_{real}} [D(x)] + 
\lambda E_{\hat{x} \hat{p}} [(||\Nabla_{\hat{x}}||_2 - 1)^2]
$$

$\hat{x}$ is a random point lying on the line between $x$ and $\tilde{x}$. 

In the experiment, GAN training with gradient penalties has faster convergence speed than that with weight clipping.
In the image generation and language modeling task, the models trained with the proposed method often gives better results 
than competing methods. 




### References

[1] Arora, Sanjeev, et al. “Generalization and Equilibrium in Generative Adversarial Nets (GANs).” arXiv preprint arXiv:1703.00573 (2017).

[2] Gulrajani, Ishaan, et al. “Improved Training of Wasserstein GANs.” arXiv preprint arXiv:1704.00028 (2017).



Use the excerpt_separator to mark the end of the short intro
(that's all that show's up on the homepage)

# This is a first level heading

## This is a second level heading

## This is a third level heading

> This is a
> blockquote
> with
>
> two paragraphs

This is a list:
* A
* B
* C

If your list items span multiple paragraphs, intend the items with three spaces.
Here the list is enumerated.

1.   This is the first sentence.

     This is the second sentence.

2.   And so on...

3.   And so forth...

This is **bold text**.
This is _italic text_.
This is ~~strikeout text~~.

This is an inline equation: $$a^2 + b^2 = c^2$$. You have to use two
dollar signs to open and close, not one like in Latex.
To have a centered equation, write it as a pararaph that starts and
ends with the two dollar signs:

$$
p(\theta \, | \, y) \propto p(\theta) \, 
p(y \, | \, \theta).
$$

I don't think you can do align blocks yet.

This is `inline  code`. 
Code blocks are intended paragraphs with four spaces:

```python
F = lambda n: ((1+np.sqrt(5))**n - (1-np.sqrt(5))**n) / (2**n * np.sqrt(5))
```
This is a figure. Note that `site.base_url` refers to the homepage.
In this case, `abc.png` is located in the `img` folder under root.

![ABC]({{site.base_url}}/img/abc.png)

### References
I've just been copying and pastying references as follows: 

[1] Meeds, Edward, Robert Leenders, and Max Welling. "Hamiltonian ABC." _arXiv preprint arXiv:1503.01916_ (2015). [link](http://arxiv.org/pdf/1503.01916)
...

### Footnotes
Here's my trick for footnotes. You can write HTML inside markdown, so I just create a
div with the id footnotes and then add a link[<sup>1</sup>](#footnotes)

<div id="footnotes"></div>
1. like this.
2. ...
