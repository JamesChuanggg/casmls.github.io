---
layout: post
title: "GAN: the new progresses"
categories: general
author: scott linderman
excerpt_separator: <!--more-->
comments: true
---

In this week's session, we read two papers about GANs and learn new progresses on GAN training. The first paper 
"Generalization and Equilibrium in Generative Adversarial Nets" is a theoretical investigation of GANs (Arora et al. [1]), 
and the second paper "Improved Training of Wasserstein GANs" gives an new training method of Wasserstein GAN 
(Gulrajani et al. [2]). 
<!--more-->

# GAN and Wasserstein Gan
GAN training is a two-player game in which the generator tries to fool the discriminator by learning a distribution 
similar with the data distribution while the discriminator tries to distinguish which distribution each sample is from. 
We say the generator "wins" when the discriminator performs no better than random guess. 

min_{u} max_v E_{x \sim p_{data}} [\log D_v(x)] + E_{ h \sim p_{normal}} [\log (1 - D_v ( G_u(h) ) ]
$$

In another understanding, the best discrininator gives a divergence measure between two distributions, $p_data$ and 
$G(h), h \sim normal$. If we have $p_{real}(x)$ and the discrinator is allowed to be any function, then the generator is 
optimized to minimize the Jesen-Shannon divergence between p_{real} and G_u(h). 

People then use Wasserstein distance to measure the divergence between two distributions (See our previous post on 
Wasserstein GAN). The GAN training objective with Wasserstein distance is 
$$
sup_{f: 1 Lipschitz}  E_{x \sim p_{\mu}}[f(x)] - E_{x \sim p_{\nu}}[f(x)] 
$$
Here the $f$ is the discriminator and takes the form of neural network. It is learned from GAN training. 


The first paper works on the following problems:
1. The distance measure is defined on distributions, but it is calculated from finite samples.
2. Can the training reach equilibrium?
3. What does it mean whether when the equilibrium is reached?

The second paper works on the problem of 
4. how to limit the discriminator in the 1 Lipschitz space. 


# Generalization of the distance measure 

When sample size is not large enough, the distance of a distribution and the empirical distribution of its samples 
is non-zero. 

The two empirical distributions of two samples from the same distribution have a distance greater than zero. 

New distance measure neural network divergence 

d(NN)()

Then the paper shows the distance calculated on empirical distributions is similar to that calculated on real 
applications when the sample size is large. 


# equilibrium

Intuition: Powerful generator can is able to win, since it can arbitrarily minimize the ditance between the generative 
distribution and the target distribution. 

A less powerful generator can also win the game, but it need to be more powerful enough. 

The payoff of the game between the generator and the discriminator, 
F(u, v)

Mixed strategy always admits equilibrium. 

However, intinite mixture is not computable, so the paper proposed $\epsilon$-approximate equilibrium. 


The paper shows that, given enough mixtures of generator and discriminators, the generator can approximately win 
the game.

# MIX+GAN: Mixture of generators and discriminators

Minimize $T$ generators and their mixture weights, $T$ discrinimators and their mixture weights.  



# Wasserstein GAN training with gradient penalty (Paper 2)

The paper is based on the nice result that the optimal discriminator (called "critic" in the paper) has 
gradients with norm 1 almost everywhere. Here the gradient is  with respect to $x$, not the parameter of 
the discriminator. 

Gradient Clipping does not work very well for the following reasons. 

1. The optimizer with gradient clipping searches the discriminator in a space smaller than 1-Lipschitz, 
so it biases the discriminator toward simpler functions. 

2. Clipped gradients vanishe or explode as it backpropagate through network layers. 


The theoretical result of the gradient and the drawback of gradient clipping motivates the new method, gradient penalty, 
in the paper. The discriminator gets a penalty if the norm of its gradient is not 1. The objective is 

$$
L = \mathbb{E}_{\tilde{x} \sim p_G}[D(\tilde{x})] - E_{x \sim p_{real}} [D(x)] + 
\lambda E_{\hat{x} \hat{p}} [(||\Nabla_{\hat{x}}||_2 - 1)^2]
$$

$\hat{x}$ is a random point lying on the line between $x$ and $\tilde{x}$. 

In the experiment, GAN training with gradient penalties has faster convergence speed than that with weight clipping.
In the image generation and language modeling task, the models trained with the proposed method often gives better results 
than competing methods. 






Use the excerpt_separator to mark the end of the short intro
(that's all that show's up on the homepage)

# This is a first level heading

## This is a second level heading

## This is a third level heading

> This is a
> blockquote
> with
>
> two paragraphs

This is a list:
* A
* B
* C

If your list items span multiple paragraphs, intend the items with three spaces.
Here the list is enumerated.

1.   This is the first sentence.

     This is the second sentence.

2.   And so on...

3.   And so forth...

This is **bold text**.
This is _italic text_.
This is ~~strikeout text~~.

This is an inline equation: $$a^2 + b^2 = c^2$$. You have to use two
dollar signs to open and close, not one like in Latex.
To have a centered equation, write it as a pararaph that starts and
ends with the two dollar signs:

$$
p(\theta \, | \, y) \propto p(\theta) \, 
p(y \, | \, \theta).
$$

I don't think you can do align blocks yet.

This is `inline  code`. 
Code blocks are intended paragraphs with four spaces:

```python
F = lambda n: ((1+np.sqrt(5))**n - (1-np.sqrt(5))**n) / (2**n * np.sqrt(5))
```
This is a figure. Note that `site.base_url` refers to the homepage.
In this case, `abc.png` is located in the `img` folder under root.

![ABC]({{site.base_url}}/img/abc.png)

### References
I've just been copying and pastying references as follows: 

[1] Meeds, Edward, Robert Leenders, and Max Welling. "Hamiltonian ABC." _arXiv preprint arXiv:1503.01916_ (2015). [link](http://arxiv.org/pdf/1503.01916)
...

### Footnotes
Here's my trick for footnotes. You can write HTML inside markdown, so I just create a
div with the id footnotes and then add a link[<sup>1</sup>](#footnotes)

<div id="footnotes"></div>
1. like this.
2. ...
