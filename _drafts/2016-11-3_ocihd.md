---
layout: post
title:  "Statistical Mechanics of Optimal Inference ih High Dimensions"
categories: general
author: Kriste Krstovski 
excerpt_separator: <!--more-->
comments: true
---

On our reading list for this week we had the paper by Advani and Ganguli titled "Statistical Mechanics of Optimal Convex Inference in High Dimensions" [1]. Classical statistics gives answers to questions on the accuracy with which statitical models can be inferred in the limits where the measurement density $$\alpha$$ is $$\alpha=\frac{N}{P}\rightarrow\infinity$$. In this work the authors are concerned with the analyses of the fundamental limits in the finite $$\alpha$$ regime which is the "big data" regime where both the number of data points $$N$$ and the number of parameters $$P$$ could be very large but still finite.  

<!--more-->

# Statistical Mechanics of Optimal Convex Inference in High Dimensions

For their analysis authors use the ubiquitoues statistical inference procedure --regression: $$y_{\mu}=X_{\mu}s^{0}+\varepsilon_{\mu}$. More specifically in their analysis they consider the class of inference algorithms known in the statistics literature as M-estimators:

$$
\widehat{s}=\underset{s}{argmin}(\sum_{N}\rho(\varepsilon_{\mu})+\sum_{P}\sigma(s_{i}))
$$

In this setup $$\rho=-log(P(\varepsilon)$$ and $$\sigma=0$$ yields the ML estimate while setting $$\rho=-log(P(\varepsilon))$$ and $$\sigma=-log(P(s))$$ gives the MAP estimate. As we noted in the reading group this class of estimators doesn't include Bayesian inference of the posterior mean. 

Analysis is focused on the behaviour of the error in estimating the signal $$q_{s}=\frac{1}{P}\sum_{i=1}^{P}(\widehat{s}_{i}-s_{i}^{0})^{2}$$ and the noise $$q_{\varepsilon}=\frac{1}{N}\sum_{\mu=1}^{N}(\widehat{\varepsilon}-\varepsilon_{\mu})^{2}$$ .

## Review for the classical statistics regime where $$\alpha\rightarrow\infinity$$
In the classical statistics regime we have the Cramer-Rao (CR) bound which is the lower bound on the error of any unbiased estimator:
$$
q_{s}\geq\frac{1}{N}\frac{1}{J[\varepsilon]}=\frac{1}{\alpha}\frac{1}{J[\varepsilon]}
$$ 
Where, $$J[\varepsilon]$$ is the Fisher information:
$$
J[\varepsilon]=\left\langle \left[\frac{\partial}{\partial s^{0}}log(P(y|s^{0}))\right]^{2}\right\rangle _{y}
$$

## Statistical Mechanics
The general idea is to solve $$\widehat{s}=\underset{s}{argmin}(\sum_{N}\rho(\varepsilon_{\mu})+\sum_{P}\sigma(s_{i}))$$
for generic convex $$\rho$$ an $$\sigma$$ by: 
1.   Defining a physical system augmented with an inverse temperature
$$\beta$$ whose ground state is a solution to the problem as $$\beta\rightarrow\infty$$.
The energy in terms of $$u=s^{0}-s$$ is: 
$$
E(u)=\sum_{N}\rho(\varepsilon_{\mu}+X_{\mu j}u_{j})+\sum_{P}\sigma(s_{j}^{0}-u_{j})
$$
2.   Calculating the free energy of the system using the replica trick 
A physical system in the microcanonical ensemble minimizes E. A physical system in the canonical ensemble (held at constant temperature $$\frac{1}{\beta}$$) minimizes the free energy $$F=E-TS=\left\langle -log(Z)\right\rangle =\left\langle -log(\int due^{-\beta E(u)})\right\rangle $$.
3.   Taking the $\beta\rightarrow\infty$ limit.

The general idea of the replica trick is to calculate an average over a polynomial of Z
instead of an intractable function of Z. The free energy is calculated using:
$$
F=-\left\langle logZ\right\rangle =\underset{n\rightarrow0}{lim}\left\langle \frac{Z^{n}-1}{n}\right\rangle =\underset{n\rightarrow0}{lim}\left\langle \frac{\partial Z^{n}}{\partial n}\right\rangle 
$$
Where $$Z^{n}$$ is the partition function of n "copies" of the system. The integral over $$X$$, $$s^{0}$$ and $$\varepsilon$$ couples these copies. This interpretation is valid for integer $$n$$, but the formula is taken to hold for continuous $$n$$

We get:

$$
\left\langle Z^{n}\right\rangle =\left\langle \int\underset{a=1}{\overset{n}{\Pi}}du^{a}e^{-\underset{a}{\sum}\beta E(u^{a})}\right\rangle =\int\left\langle \int\underset{a,b=1}{\overset{n}{\Pi}}dQ_{ab}d\widehat{Q}_{ab}e^{-Nf(Q,\widehat{Q})}\right\rangle 
$$
Where:
$$
Q_{ab}\delta_{\mu\nu} & = & \left\langle X_{\mu}\cdot u^{a}X_{\nu}\cdot u^{b}\right\rangle _{X}
$$

In the replica symmetric (RS) approximation the assumption is that the $$2n^{2}$$ variables $$Q_{ab},\widehat{Q}_{ab}$$
can be replaced by 4 variables in the form $$Q=diag(\Delta)+ones(Q_{0})$$.
This is a mean field assumption on the interaction between the replicas:
$$r_{ij}x_{i}x_{j}\approx\sum r_{1}x_{i}^{2}+r_{0}x_{i}$$$
The results are 

$$ 
q_{d}=\frac{\left\langle M'_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}})^{2}\right\rangle _{\varepsilon_{q_{s}}}}{\alpha\left\langle M''_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}})\right\rangle _{\varepsilon_{q_{s}}}^{2}} 
& 
q_{s}=\left\langle (\widehat{s}(s_{q_{s}}^{0})-s^{0})^{2}\right\rangle _{s_{q_{d}}^{0}} \\

1-\frac{1}{\alpha}\frac{\lambda_{\rho}}{\lambda_{\sigma}}=\left\langle
\widehat{\varepsilon}'(\varepsilon_{q_{s}})\right\rangle _{\varepsilon_{q_{s}}} 
& 
\frac{\lambda_{\rho}}{\lambda_{\sigma}}=\left\langle \widehat{s}'(s_{q_{d}}^{0})\right\rangle _{s_{q_{d}}^{0}}
$$

Where 
$$
\varepsilon_{q_{s}}=\varepsilon+\sqrt{q_{s}}z_{\varepsilon},s_{q_{s}}^{0}=s^{0}+\sqrt{q_{d}}z_{s}
$$

$$
\widehat{\varepsilon}(\varepsilon_{q_{s}})=P_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}}),\widehat{s}(s_{q_{d}}^{0})=P_{\lambda_{\sigma}}[\sigma](\varepsilon_{q_{d}})
$$

$$
\left(\mathcal{E}_{gen}=\left\langle \varepsilon_{q_{s}}^{2}\right\rangle =q_{s}+\left\langle \varepsilon\right\rangle ^{2},\mathcal{E}_{train}=\left\langle \widehat{\varepsilon}(\varepsilon_{q_{s}})^{2}\right\rangle \right)
$$


### Inference Without Prior Information 
If we cannot utilize prior information , then we set $$\sigma=0$$ which gives $$\widehat{s}(s_{q_{d}}^{0})=s_{q_{d}}^{0}$$
and the equations reduce to:
$$
q_{s}=q_{d}=\frac{1}{\alpha}\frac{\left\langle (M'[\rho])^{2}\right\rangle _{\varepsilon_{q_{s}}}}{\left\langle M''[\rho]\right\rangle _{\varepsilon_{q_{s}}}^{2}}
$$

Optimal error in the finite $\alpha$ regime
What is the optimal error in this settiing?
We can rewrite the equations as:
$$
\alpha\left\langle (\lambda_{\rho}M_{\lambda_{\rho}}[\rho]'(\varepsilon_{q_{s}}))^{2}\right\rangle -q_{s}=0\equiv A
$$

$$
\alpha\left\langle \lambda_{\rho}M_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}})''\right\rangle -1=0\equiv B
$$

Then after some algebra that eliminates the $$\lambda_{\rho}M_{\lambda_{\rho}}[\rho]'(\varepsilon_{q_{s}})$$
terms we can obtain:
$$
q_{s}^{opt}=\frac{1}{\alpha}\frac{1}{J[\varepsilon_{q^{opt}}]}\geq\frac{1}{(\alpha-1)J[\varepsilon]}
$$
Notice that the bound is higher than the Cramer-Rao bound - due to the additional noise added to $\varepsilon$ from z. 

Optimal cost

One can look for $$\rho_{opt}$$ that achieves $$q_{opt}$$ by adding these constraints to a Lagrangian that minimizes the error:
$$
L=q_{s}-\lambda A-\mu B
$$

Then by solving the Euler Lagrange equation $$\frac{\partial L}{\partial r}-\frac{\partial}{\partial y}\frac{\partial L}{\partial r'}=0$$ and inverting the Moreau envelope $$M[f]=g\Leftrightarrow f=-M[-g]$$
we obtain:
$$
\rho_{opt}=-M_{q_{opt}}[log(f_{q_{opt}})]=-M_{q_{opt}}[-E_{q_{opt}}]
$$

Where $f_{q_{opt}}$is the PDF of $\varepsilon_{q_{opt}}$. 


Figure 4 from the paper (shown below) gives a comparison summary of the generalization (a) and training (b) error of the optimal unregularized M-estimator with ML and quadratic loss functions. It shows overfitting as $$\alpha\rightarrow1$$ 
Optimizing over $$q_{s}$$ is analogous to optimizing over $$\mathcal{E}_{gen}=q_{s}+\left\langle \varepsilon^{2}\right\rangle $$ and not $$\mathcal{E}_{train}=\left\langle \widehat{\varepsilon}(\varepsilon_{q_{s}})^{2}\right\rangle$$.

<!---![Fig. 4 in the paper](/img/ocihg_figure4.png)--->
<img src="/img/ocihd_figure4.png" alt="Generalization and training error comparison of the optimal unregularized M-estimator with ML and quadratic loss functions", width=500/>

### Inference With Prior Information

We start with the often used quadratic loss and regularization: $$\rho(x)=\frac{1}{2}x^{2}$$ , $$\sigma(x)=\frac{1}{2}\gamma x^{2}$$

As we have seen, for $$f=\frac{1}{2}\gamma x^{2}$$ we have the simplification
$$P[f](x)=\underset{y}{argmin}[\frac{1}{2\lambda}(x-y)^{2}+\frac{1}{2}\gamma y^{2}]=\frac{x}{1+\gamma\lambda}$$

The 4 equations reduce to:

$$
\frac{q_{d}}{\lambda_{\sigma}^{2}}=\frac{\alpha}{(1+\lambda_{\rho})^{2}}\left\langle \varepsilon_{q_{s}}^{2}\right\rangle  
& 
q_{s}=\left\langle \left(\frac{s^{0}+z\sqrt{q_{d}}}{1+\gamma\lambda_{\sigma}}-s^{0}\right)^{2}\right\rangle 
\\
\lambda_{\sigma}=\frac{1+\lambda_{\rho}}{\alpha} 
& \lambda_{\rho}=\frac{\lambda_{\sigma}}{1+\gamma\lambda_{\sigma}}
$$

From the first two equations one obtains:

$$
\overline{q}_{s} & = & \frac{q_{s}}{\left\langle s_{0}^{2}\right\rangle }=\frac{\phi+\alpha(\gamma\lambda_{\sigma})^{2}}{\alpha(1+\gamma\lambda_{\sigma})^{2}-1}
$$

The optimal regulatization is given by solving $$\frac{\partial\overline{q}_{s}}{\partial\gamma}=0$
which gives $\gamma=\phi$$. This means that high SNR requires little regularization. Plugging this into the remaining equations, we solve for $$\overline{q}_{opt}$$:
$$
\overline{q}_{s}^{Quad}=\frac{1-\alpha-\phi+\sqrt{(\phi+\alpha-1)^{2}+4\phi}}{2}
$$

This exhibits a phase transition, which in high SNR reduces to:
$$
\overline{q}_s^{Quad}=\begin{cases}
1-\alpha & \alpha<1 \\
\frac{1}{\sqrt{SNR}} & \alpha=1\\
\frac{1}{SNR(\alpha-1)} & \alpha>1
\end{cases}
$$

Shown below is Figure 5 from the paper where we observe that $$\overline{q}_s^{Quad}$$ approaches a constant when $$\alpha<1$$. 

<!---![Fig. 5 in the paper](/img/ocihg_figure5.png)--->
<img src="/img/ocihd_figure5.png" alt="Optimal quadratic error vs. SNR and RMT interpretation", width=500/>

### Optimal Inference with non-Gaussian Signal and Noice
Generalizing the unregularized case, the optimal cost and regularizer
is found to be:

$$
\rho_{opt}=-M_{q_{s}^{opt}}[log(P_{\varepsilon_{q_{s}^{opt}}})]
$$

$$
\sigma_{opt}=-M_{q_{d}^{opt}}[log(P_{s_{q_{d}^{opt}}}]
$$

Where: 
$$
q_{d}^{opt}=\frac{1}{\alpha J[\varepsilon_{q_{s}^{opt}}]}, q_{s}^{opt}=q_{s}^{MMSE}(q_{d}^{opt})
$$

## References
[1] Madhu Advani and Syrya Ganguli. “Statistical mechanics of optimal convex inference in high dimensions”, In Physical Review X, 6:031034 (2016). [link](http://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031034)


