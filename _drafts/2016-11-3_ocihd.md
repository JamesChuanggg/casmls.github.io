---
layout: post
title:  "Statistical Mechanics of Optimal Inference ih High Dimensions"
categories: general
author: Kriste Krstovski 
excerpt_separator: <!--more-->
comments: true
---

On our reading list for this week we had the paper by Advani and Ganguli titled "Statistical Mechanics of Optimal Convex Inference in High Dimensions" [1]. Classical statistics analyzes the performance and gives answers to questions on the limits of the accuracy with which statitical models can be inferred in the limits where the measurement density $$\alpha$$ is $$\alpha=\frac{N}{P}\rightarrow\infinity$$. In this work the authors are concerned with the analyses of the fundamental limits in the finite $$\alpha$$ regime which is the "big data" regime where both the number of data points $$N$$ and the number of parameters $$P$$ could be very large.  

<!--more-->

# Statistical Mechanics of Optimal Convex Inference in High Dimensions

For their analysis authors use the ubiquitoues statistical inference procedure --regression: $$y_{\mu}=X_{\mu}s^{0}+\varepsilon_{\mu}$. More specifically in their analysis they consider the class of inference algorithms known in the statistics literature as M-estimators:

$$
\widehat{s}=\underset{s}{argmin}(\sum_{N}\rho(\varepsilon_{\mu})+\sum_{P}\sigma(s_{i}))
$$

In this setup setting $$\rho=-log(P(\varepsilon)$$ and $$\sigma=0$$ yields the ML estimate while setting $$\rho=-log(P(\varepsilon))$$ and $$\sigma=-log(P(s))$$ gives the MAP estimate. As we noted in the reading group this class doesn't include Bayesian inference of the posterior mean. 

Analysis is focused on the behaviour of the error in estimating the signal $$q_{s}=\frac{1}{P}\sum_{i=1}^{P}(\widehat{s}_{i}-s_{i}^{0})^{2}$$ and the noise $$q_{\varepsilon}=\frac{1}{N}\sum_{\mu=1}^{N}(\widehat{\varepsilon}-\varepsilon_{\mu})^{2}$$ .

## Review for the classical statistics regime where $$\alpha\rightarrow\infinity$$
In the classical statistics regime we have the Cramer-Rao (CR) bound which is the lower bound on the error of any unbiased estimator:
$$
q_{s}\geq\frac{1}{N}\frac{1}{J[\varepsilon]}=\frac{1}{\alpha}\frac{1}{J[\varepsilon]}
$$ 
Where, $$J[\varepsilon]$$ is the Fisher information:
$$
J[\varepsilon]=\left\langle \left[\frac{\partial}{\partial s^{0}}log(P(y|s^{0}))\right]^{2}\right\rangle _{y}
$$

## Statistical Mechanics
The general idea is to solve $$\widehat{s}=\underset{s}{argmin}(\sum_{N}\rho(\varepsilon_{\mu})+\sum_{P}\sigma(s_{i}))$$
for generic convex $$\rho$$ an $$\sigma$$ by: 
1.   Defining a physical system augmented with an inverse temperature
$$\beta$$ whose ground state is a solution to the problem as $$\beta\rightarrow\infty$$.
The energy in terms of $$u=s^{0}-s$$ is: 
$$
E(u)=\sum_{N}\rho(\varepsilon_{\mu}+X_{\mu j}u_{j})+\sum_{P}\sigma(s_{j}^{0}-u_{j})
$$

2.   Calculating the free energy of the system using the replica trick 
A physical system in the microcanonical ensemble minimizes E. A physical system in the canonical ensemble (held at constant temperature $$\frac{1}{\beta}$$) minimizes the free energy $$F=E-TS=\left\langle -log(Z)\right\rangle =\left\langle -log(\int due^{-\beta E(u)})\right\rangle $$.
3.   Taking the $\beta\rightarrow\infty$ limit.

The general idea of the replica trick is to calculating an average over a polynomial of Z
instead of an intractable function of Z. The free energy is calculated using:
$$
F=-\left\langle logZ\right\rangle =\underset{n\rightarrow0}{lim}\left\langle \frac{Z^{n}-1}{n}\right\rangle =\underset{n\rightarrow0}{lim}\left\langle \frac{\partial Z^{n}}{\partial n}\right\rangle 
$$
Where $$Z^{n}$$ is the partition function of n "copies" of the system. The integral over $$X$$, $$s^{0}$$, $$\varepsilon$$ couples these copies. This interpretation is valid for integer n, but the formula is taken to hold for continuous $$n$$

After some algebra one can write 

\[
\left\langle Z^{n}\right\rangle =\left\langle \int\underset{a=1}{\overset{n}{\Pi}}du^{a}e^{-\underset{a}{\sum}\beta E(u^{a})}\right\rangle =\int\left\langle \int\underset{a,b=1}{\overset{n}{\Pi}}dQ_{ab}d\widehat{Q}_{ab}e^{-Nf(Q,\widehat{Q})}\right\rangle 
\]

\begin{eqnarray*}
Q_{ab}\delta_{\mu\nu} & = & \left\langle X_{\mu}\cdot u^{a}X_{\nu}\cdot u^{b}\right\rangle _{X}
\end{eqnarray*}

The RS assumption is that the $2n^{2}$ variables $Q_{ab},\widehat{Q}_{ab}$
can be replaced by 4 in the form $Q=diag(\Delta)+ones(Q_{0})$ ($\widehat{Q}=...$).

This is a mean field assumption on the interaction between the replicas
- $r_{ij}x_{i}x_{j}\approx\sum r_{1}x_{i}^{2}+r_{0}x_{i}$

- The calculation, aside from the $n\rightarrow0$ limit, uses a bunch
of properties of Gaussian integrals and delta functions 

- The results are 

\[
\begin{array}{cc}
q_{d}=\frac{\left\langle M'_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}})^{2}\right\rangle _{\varepsilon_{q_{s}}}}{\alpha\left\langle M''_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}})\right\rangle _{\varepsilon_{q_{s}}}^{2}} & q_{s}=\left\langle (\widehat{s}(s_{q_{s}}^{0})-s^{0})^{2}\right\rangle _{s_{q_{d}}^{0}}\\
1-\frac{1}{\alpha}\frac{\lambda_{\rho}}{\lambda_{\sigma}}=\left\langle \widehat{\varepsilon}'(\varepsilon_{q_{s}})\right\rangle _{\varepsilon_{q_{s}}} & \frac{\lambda_{\rho}}{\lambda_{\sigma}}=\left\langle \widehat{s}'(s_{q_{d}}^{0})\right\rangle _{s_{q_{d}}^{0}}
\end{array}
\]

where 
\[
\varepsilon_{q_{s}}=\varepsilon+\sqrt{q_{s}}z_{\varepsilon},s_{q_{s}}^{0}=s^{0}+\sqrt{q_{d}}z_{s}
\]

\[
\widehat{\varepsilon}(\varepsilon_{q_{s}})=P_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}}),\widehat{s}(s_{q_{d}}^{0})=P_{\lambda_{\sigma}}[\sigma](\varepsilon_{q_{d}})
\]

\[
\left(\mathcal{E}_{gen}=\left\langle \varepsilon_{q_{s}}^{2}\right\rangle =q_{s}+\left\langle \varepsilon\right\rangle ^{2},\mathcal{E}_{train}=\left\langle \widehat{\varepsilon}(\varepsilon_{q_{s}})^{2}\right\rangle \right)
\]


\subsubsection{Remark - calculating joint distributions}

From these one can derive joint distributions for $(\widehat{s}_{i},s^{0}),(\widehat{\varepsilon}_{i},\varepsilon^{0})$.
These aren't given explicitly but are derived as follows

-distributions of $\varepsilon_{q_{s}},s_{q_{d}}^{0}$ are known

- $\widehat{s}(s_{q_{d}}^{0}),\widehat{\varepsilon}(\varepsilon_{q_{d}})$
are known and can be used to construct histograms of $P_{MF}(s_{i}^{0},\widehat{s}_{i}),P_{MF}(\varepsilon_{i},\widehat{\varepsilon}_{i})$. 

\subsubsection{Why do proximal maps show up?}

Before taking the $\beta\rightarrow\infty$ limit, the $\beta$ results
(2.1) depend on distributions of the form

\[
P_{\sigma}(s|s_{q_{d}}^{0})\propto exp(-\beta\left[\frac{s-s_{q_{d}}^{0}}{2\lambda_{\sigma}}+\sigma(s)\right])
\]
An integral over the above distribution at the $\beta\rightarrow\infty$
limit corresponds to replacing s with $\widehat{s}=P(s)$. (with a
similar dist giving $\widehat{\varepsilon}$) 



### Inference Without Prior Information 
If we cannot utilize prior information , then we set $$\sigma=0$$ which gives $$\widehat{s}(s_{q_{d}}^{0})=s_{q_{d}}^{0}$$
and the equations reduce to:
$$
q_{s}=q_{d}=\frac{1}{\alpha}\frac{\left\langle (M'[\rho])^{2}\right\rangle _{\varepsilon_{q_{s}}}}{\left\langle M''[\rho]\right\rangle _{\varepsilon_{q_{s}}}^{2}}
$$

Optimal error in the finite $\alpha$ regime
What is the optimal error in this settiing?
We can rewrite the equations as:
$$
\alpha\left\langle (\lambda_{\rho}M_{\lambda_{\rho}}[\rho]'(\varepsilon_{q_{s}}))^{2}\right\rangle -q_{s}=0\equiv A
$$

$$
\alpha\left\langle \lambda_{\rho}M_{\lambda_{\rho}}[\rho](\varepsilon_{q_{s}})''\right\rangle -1=0\equiv B
$$

Then after some algebra that eliminates the $$\lambda_{\rho}M_{\lambda_{\rho}}[\rho]'(\varepsilon_{q_{s}})$$
terms we can obtain:
$$
q_{s}^{opt}=\frac{1}{\alpha}\frac{1}{J[\varepsilon_{q^{opt}}]}\geq\frac{1}{(\alpha-1)J[\varepsilon]}
$$
Notice that the bound is higher than the Cramer-Rao bound - due to the additional noise added to $\varepsilon$ from z. 

Optimal cost

One can look for $$\rho_{opt}$$ that achieves $$q_{opt}$$ by adding these constraints to a Lagrangian that minimizes the error:
$$
L=q_{s}-\lambda A-\mu B
$$

Then by solving the Euler Lagrange equation $$\frac{\partial L}{\partial r}-\frac{\partial}{\partial y}\frac{\partial L}{\partial r'}=0$$ and inverting the Moreau envelope $$M[f]=g\Leftrightarrow f=-M[-g]$$
we obtain:
$$
\rho_{opt}=-M_{q_{opt}}[log(f_{q_{opt}})]=-M_{q_{opt}}[-E_{q_{opt}}]
$$

Where $f_{q_{opt}}$is the PDF of $\varepsilon_{q_{opt}}$. 

Figure 4 from the paper (shown below) gives a comparison summary of the generalization (a) and training (b) error of the optimal unregularized M-estimator with ML and quadratic loss functions.n

<!---![Fig. 4 in the paper](/img/ocihg_figure4.png)--->
<img src="/img/ocihd_figure4.png" alt="Generalization and training error comparison of the optimal unregularized M-estimator with ML and quadratic loss functions" width="400"/>


### Inference With Quadratic Prior and Cost




## References
[1] Madhu Advani and Syrya Ganguli. “Statistical mechanics of optimal convex inference in high dimensions”, In Physical Review X, 6:031034 (2016). [link](http://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031034)


