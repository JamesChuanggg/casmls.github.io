---
layout: post
title: "Implicit Generative Models - What are you GAN-na do? (and other topics)"
categories: general
author: andrew davison
excerpt_separator: <!--more-->
comments: true
---

A few weeks ago, Dar lead our discussion about the following two (mostly unrelated) papers: "Learning in Implicit Generative Models"
by Mohamed and Lakshminarayanan (ref) and "A Variational Perspective on Accelerated Methods in Optimization" by Wibisono et al. (ref).
The former gives a good overview of techniques for learning in implicit generative models, and has links to 
many of the areas we've discussed this past year, which I'll highlight throughout. The latter is a more theoretical paper investigating 
the nature of accelerated
gradient methods and the natural scope for such concepts. Here we'll introduce and motivate some of the mathematical aspects used in the
paper, and talk about some interesting links of these algorithms to deep learning brought up by Dar.

<!--more-->

# Learning in Implicit Generative Models

Usually, when thinking of probabilistic methods we think of an explicit parametric specification of the distribution of a random
variable $$X$$ in which we directly specify a log-likelihood $$\log q_{\theta}(x)$$ with $$\theta$$ indexing 
a family of possible distributions, with the goal of learning $$theta$$ from the log-likelihood.
In contrast, it is frequently more natural to specify models implicitly via a latent variable 
$$Z \in \mathbb{R}^m$$ which is very easy to generate (e.g multivariate gaussian random variables) 
and then we transform via a deterministic function $$\mathcal{G}_{\theta} : \mathbb{R}^m \mapsto \mathbb{R}^n$$, again with
$$\theta$$ indexing a family of such functions.

But why is the problem of learning $$\theta$$ different here? Namely, it boils down to that the density $$q_{\theta}(x)$$ of $$X$$ 
can be highly intractable for a number of different reasons. Writing $$q(z)$$ for the density of the latent variable $$Z$$, we have
that 

$$ q_{\theta}(x) = \frac{ \partial }{\partial x_1 } \cdots \frac{ \partial }{\partial x_d } \int_{z : \mathcal{G}_{\theta}(z) \leq x } 
q(z) \, dz $$

where for $$z, z' \in \mathbb{R}^m$$, $$z \leq z'$$ if and only if $$z_i \leq z'_i$$ for $$i = 1, \ldots, m$$. Although for certain
types of $$\mathcal{G}$$ (e.g when it is easily invertible) this integral is tractable, frequently we have scenarios where $$\mathcal{G}$$ 
is non-linear and is specified by a deep network. If $$d = m$$, then it could be possible to use normalizing flows (which we've
earlier discussed in the context of [variational inference](https://casmls.github.io/general/2016/09/25/normalizing-flows.html)) to 
allow for this integral to be evaluated; however if $$d > m$$ or $$\mathcal{G}$$ is not invertible, then we would have to proceed 
differently. We would then likely encounter several compounding issues:

* The integration region itself may be very hard to determine.
* Even if we knew the integration region, the integral itself may then be computationally demanding, depending on the geometry of
the integration region, it's dimension, and the form of the density $$q(z)$$.
* Even if we can calculate the integral, we then have to take $$d$$ partial derivatives, which in a high-dimensional scenario
is very challenging. 

We would therefore like to have a _likelihood-free_ approach to inference, for which there have been various solutions proposed, namely
generative adversarial networks (GANs) (ref) and classifier ABC (ref). The contribution of this paper is to bring together 
the methods and ideas for such inference methdods, often known only in isolation. The paper focuses mainly on GANs, so for those interested
in the basic ideas behind ABC (**A**pproximate **B**ayesian **C**omputation), see [Scott's blog post](https://casmls.github.io/general/2016/10/02/abc.html)
from when we discussed some ways of extending it to higher dimensions. 

_**Notation**: The data are denoted by a random variable $$X$$, whose true data density is $$p^*(x)$$, with intractable model density 
$$q_{\theta}(x)$$. $$\theta$$ is reserved for model parameters; $$\phi$$ is used to denote other parameters, such as for discrminator functions._

## Likelihood-free inference via density ratios and differences

## Four general approaches

## Conclusion

# A Variational Perspective on Accelerated Methods in Optimization

## This is a third level heading

> This is a
> blockquote
> with
>
> two paragraphs

This is a list:
* A
* B
* C

If your list items span multiple paragraphs, intend the items with three spaces.
Here the list is enumerated.

1.   This is the first sentence.

     This is the second sentence.

2.   And so on...

3.   And so forth...

This is **bold text**.
This is _italic text_.
This is ~~strikeout text~~.

This is an inline equation: $$a^2 + b^2 = c^2$$. You have to use two
dollar signs to open and close, not one like in Latex.
To have a centered equation, write it as a pararaph that starts and
ends with the two dollar signs:

$$
p(\theta \, | \, y) \propto p(\theta) \, 
p(y \, | \, \theta).
$$

I don't think you can do align blocks yet.

This is `inline  code`. 
Code blocks are intended paragraphs with four spaces:

```python
F = lambda n: ((1+np.sqrt(5))**n - (1-np.sqrt(5))**n) / (2**n * np.sqrt(5))
```
This is a figure. Note that `site.base_url` refers to the homepage.
In this case, `abc.png` is located in the `img` folder under root.

![ABC]({{site.base_url}}/img/abc.png)

### References
I've just been copying and pastying references as follows: 

[1] Learning/GAN's paper [link](http://arxiv.org/pdf/1503.01916)

[2] Bregman divergence paper [link](http://arxiv.org/pdf/1503.01916)

[3] Original GAN paper

[4] Classifier ABC paper
...

### Footnotes
Here's my trick for footnotes. You can write HTML inside markdown, so I just create a
div with the id footnotes and then add a link[<sup>1</sup>](#footnotes)

<div id="footnotes"></div>
1. like this.
2. ...

