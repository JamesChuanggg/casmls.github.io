---
layout: post
title: "Markdown Tutorial"
categories: general
author: Gamaleldin Elsayed
excerpt_separator: <!--more-->
comments: true
---
# Autoregressive Distribution Estimation

This week we discussed Germain et al., MADE and Uria et al., NADE papers on distribution estimation. The main approach of the two papers to estimate joint distributions of data is similar. That is to modify the structure of the autoencoder neural networks to be consistent with autoregressive models. MADE focused on binary data cases whereas NADE extended this idea to real valued data and included more extensive experiments.

<!--more-->

# Distribution Estimation
**Set up** 
Given a set of examples $$\{x^(t)\}_t=1^T$$, where $$x \in R^D$, the goal is to estimate a joint distribution $$p(x)$$. This distribution quantifies the statistical properties of data and can be used as a generative model to sample new data. This generative model is useful in many applications such as classifications, data denoising or missing data completion. This problem is relatively easy if the dimensionality of the data is low (_e.g.,_ estimate distribution from many examples of real valued numbers). However, in cases when data is high dimensional (_e.g.,_ space of pixels of an image), estimation of the data distribution becomes difficult. The main problem is that as dimensionality increases the volume of the space the distribution $$p(x)$$ needs to cover increases exponentially, which make it harder for a finite datasets to give a clear picture of the statistical properties of that space. 

# Autoencoder neural networks:
One powerful idea to estimate the distribution of data is to utilize the power of neural networks as function approximators. In this setting, a neural network learns a feedforward representation of the input data examples in its hidden layers with the goal of regenerating the input data as accurate as possible. These hidden representations can thus reveal the statistical structures of the data generative distribution.  For example, to learn the representation of binary data using a one-layer network, we can frame the problem as follows:

$$
h(x) = g(b + Wx)
\hat x = sigmoid(c + V h(x))
$$
where $$g$$ is the hidden layer nonlinear activation function, W and V are network input-to-hidden and hidden-to-output weights, respectively, and b and c are the bias terms.
This autoencoder can thus be trained to fined the parameters $$\theta = {W, V, b, c}$$ that regenerates the input as accurate as possible. The typical loss function used is the cross-entropy:
$$
l(x) = \sum_{d=1}^D -x_d log(\hat {x}_d) - (1-x_d) log(1 - \hat {x}_d)
$$
This function is thought to represent the log likelihood function. Howver, in the general case of all-to-all connections it is in fact not a proper loglikelihood.







# This is a first level heading

## This is a second level heading

## This is a third level heading

> This is a
> blockquote
> with
>
> two paragraphs

This is a list:
* A
* B
* C

If your list items span multiple paragraphs, intend the items with three spaces.
Here the list is enumerated.

1.   This is the first sentence.

     This is the second sentence.

2.   And so on...

3.   And so forth...

This is **bold text**.
This is _italic text_.
This is ~~strikeout text~~.

This is an inline equation: $$a^2 + b^2 = c^2$$. You have to use two
dollar signs to open and close, not one like in Latex.
To have a centered equation, write it as a pararaph that starts and
ends with the two dollar signs:

$$
p(\theta \, | \, y) \propto p(\theta) \, 
p(y \, | \, \theta).
$$

I don't think you can do align blocks yet.

This is `inline  code`. 
Code blocks are intended paragraphs with four spaces:

```python
F = lambda n: ((1+np.sqrt(5))**n - (1-np.sqrt(5))**n) / (2**n * np.sqrt(5))
```
This is a figure. Note that `site.base_url` refers to the homepage.
In this case, `abc.png` is located in the `img` folder under root.

![ABC]({{site.base_url}}/img/abc.png)

### References
I've just been copying and pastying references as follows: 

[1] Meeds, Edward, Robert Leenders, and Max Welling. "Hamiltonian ABC." _arXiv preprint arXiv:1503.01916_ (2015). [link](http://arxiv.org/pdf/1503.01916)
...

### Footnotes
Here's my trick for footnotes. You can write HTML inside markdown, so I just create a
div with the id footnotes and then add a link[<sup>1</sup>](#footnotes)

<div id="footnotes"></div>
1. like this.
2. ...

