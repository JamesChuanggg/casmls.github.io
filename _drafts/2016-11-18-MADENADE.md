---
layout: post
title: "Autoregressive Distribution Estimation"
categories: general
author: Gamaleldin Elsayed
excerpt_separator: <!--more-->
comments: true
---
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

This week we discussed Germain et al., MADE and Uria et al., NADE papers on distribution estimation. The main approach of the two papers to estimate joint distributions of data is similar. That is to modify the structure of the autoencoder neural networks to be consistent with autoregressive models. MADE focused on binary data cases whereas NADE extended this idea to real valued data and included more extensive experiments.

<!--more-->

# Distribution Estimation
**Set up** 
Given a set of examples $$\{x^{(t)}\}_{t=1}^T$$, where $$x \in R^D$$, the goal is to estimate a joint distribution $$p(x)$$. This distribution quantifies the statistical properties of data and can be used as a generative model to sample new data. This generative model is useful in many applications such as classifications, data denoising or missing data completion. This problem is relatively easy if the dimensionality of the data is low (_e.g.,_ estimate distribution from many examples of real valued numbers). However, in cases when data is high dimensional (_e.g.,_ space of pixels of an image), estimation of the data distribution becomes difficult. The main problem is that as dimensionality increases the volume of the space the distribution $$p(x)$$ needs to cover increases exponentially, which make it harder for a finite datasets to give a clear picture of the statistical properties of that space. 

# Autoencoder neural networks:
One powerful idea to estimate the distribution of data is to utilize the power of neural networks as function approximators. In this setting, a neural network learns a feedforward representation of the input data examples in its hidden layers with the goal of regenerating the input data as accurate as possible. These hidden representations can thus reveal the statistical structures of the data generative distribution.  For example, to learn the representation of binary data using a one-layer network, we can frame the problem as follows:

$$
h(x) = g(b + Wx)\\
\hat x = sigmoid(c + V h(x))
$$

where $$g$$ is the hidden layer nonlinear activation function, W and V are network input-to-hidden and hidden-to-output weights, respectively, and b and c are the bias terms. The main advantage of this framework is that it is very flexible and easy to train to find the best parameters $$\theta = \{W, V, b, c\}$$ with stochastic gradient descent that regenerates the input as accuratly as possible. The typical loss function used is the cross-entropy:

$$
l(x) = \sum_{d=1}^D -x_d \log(\hat {x}_d) - (1-x_d) \log(1 - \hat {x}_d)
$$

This function is thought to represent the log likelihood function. However, in the general case of all-to-all connections, it is in fact not a proper loglikelihood. The main confound is that in a fully connected network the generative process of data at dimension $$d$$ depends on the input data at dimensions $$d$$. This makes it possible for the network to learn trivial generative processes such as simply copying the input data to the output (i.e., there is a trivial set of weight that will simply assign $$x_d$$ to $$\hat {x}_d$$). The essence of this problem is that the distribution identified is not truly the joint distributions (not properly normalized). One can see that in this trivial case of copying the input where the data is produced perfectly with probability one for each sample data, which makes the probabilities over all possible patterns does not sum to 1 (equal to number of training data patterns in this example).


## Distribution estimation with autoregression:
The decomposition of joint distributions to product of conditions gives a solution to the above problem of the autoencoders. In general, the joint distribution over data can be written as follows:

$$
p(x_1, ..., x_D) = p(x_D \, | \, x_1, ..., x_{D-1}) p(x_{D-1} \, | \, x_1,..., x_{D-2}) ... p(x_1) = \prod_{d=1}^{D} p(x_d \, | \, x_{<d})
$$

Remember the main problem of the autoencoders is that $$\hat x_d$$ depends on  all $$xs$$ due to the full connections in the neural network. However, if the connections are modified to satisfy this autoregressive encoders, this will eleminate the possibilities of trivial representations and will allow the network to learn a proper joint distribution.



## Masked Autoregressive Distribution Estimation (MADE)

The main idea of MADE is to modify the connections in the autoencoder to satisfy the autoregressive property using masked weights. To make enforce that there are no dependency between $$\hat {x}_d$$ and $$x_{>d}$$, MADE make sure there is no computational paths between $$\hat {x}_d $$ and $$x_{>d}$$ by multiply the network weight by masks as follows:



# This is a first level heading

## This is a second level heading

## This is a third level heading

> This is a
> blockquote
> with
>
> two paragraphs

This is a list:
* A
* B
* C

If your list items span multiple paragraphs, intend the items with three spaces.
Here the list is enumerated.

1.   This is the first sentence.

     This is the second sentence.

2.   And so on...

3.   And so forth...

This is **bold text**.
This is _italic text_.
This is ~~strikeout text~~.

This is an inline equation: $$a^2 + b^2 = c^2$$. You have to use two
dollar signs to open and close, not one like in Latex.
To have a centered equation, write it as a pararaph that starts and
ends with the two dollar signs:

$$
p(\theta \, | \, y) \propto p(\theta) \, 
p(y \, | \, \theta).
$$

I don't think you can do align blocks yet.

This is `inline  code`. 
Code blocks are intended paragraphs with four spaces:

```python
F = lambda n: ((1+np.sqrt(5))**n - (1-np.sqrt(5))**n) / (2**n * np.sqrt(5))
```
This is a figure. Note that `site.base_url` refers to the homepage.
In this case, `abc.png` is located in the `img` folder under root.

![ABC]({{site.base_url}}/img/abc.png)

### References
I've just been copying and pastying references as follows: 

[1] Meeds, Edward, Robert Leenders, and Max Welling. "Hamiltonian ABC." _arXiv preprint arXiv:1503.01916_ (2015). [link](http://arxiv.org/pdf/1503.01916)
...

### Footnotes
Here's my trick for footnotes. You can write HTML inside markdown, so I just create a
div with the id footnotes and then add a link[<sup>1</sup>](#footnotes)

<div id="footnotes"></div>
1. like this.
2. ...

